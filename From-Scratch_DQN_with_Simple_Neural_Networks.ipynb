{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "# Define a simple environment\n",
        "class SimpleEnv:\n",
        "    def __init__(self):\n",
        "        self.state = 2  # Starting in the middle of the 1D grid\n",
        "        self.goal = 4   # The goal position\n",
        "        self.states = [0, 1, 2, 3, 4]  # 1D grid of 5 positions\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = 2  # Reset to the start position\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == 0:  # Move left\n",
        "            self.state = max(0, self.state - 1)\n",
        "        elif action == 1:  # Move right\n",
        "            self.state = min(4, self.state + 1)\n",
        "\n",
        "        if self.state == self.goal:\n",
        "            reward = 1  # Goal reached\n",
        "            done = True\n",
        "        else:\n",
        "            reward = 0  # No reward otherwise\n",
        "            done = False\n",
        "\n",
        "        return self.state, reward, done\n",
        "\n",
        "\n",
        "# Neural Network from scratch\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Initialize weights randomly\n",
        "        self.weights1 = [[random.random() for _ in range(hidden_size)] for _ in range(input_size)]  # Input to hidden\n",
        "        self.weights2 = [[random.random() for _ in range(output_size)] for _ in range(hidden_size)]  # Hidden to output\n",
        "        self.bias1 = [random.random() for _ in range(hidden_size)]  # Bias for hidden layer\n",
        "        self.bias2 = [random.random() for _ in range(output_size)]  # Bias for output layer\n",
        "\n",
        "    def relu(self, x):\n",
        "        return [max(0, xi) for xi in x]  # ReLU activation function\n",
        "\n",
        "    def relu_derivative(self, x):\n",
        "        return [1 if xi > 0 else 0 for xi in x]  # Derivative of ReLU\n",
        "\n",
        "    def forward(self, state):\n",
        "        # Forward pass: state -> hidden -> output\n",
        "        self.input = state\n",
        "        self.hidden = self.relu([sum([self.input[i] * self.weights1[i][j] for i in range(len(self.input))]) + self.bias1[j] for j in range(len(self.weights1[0]))])\n",
        "        self.output = [sum([self.hidden[i] * self.weights2[i][j] for i in range(len(self.hidden))]) + self.bias2[j] for j in range(len(self.weights2[0]))]\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, target, learning_rate):\n",
        "        # Backpropagation (basic implementation)\n",
        "        output_error = [(target[i] - self.output[i]) for i in range(len(target))]  # Error in output\n",
        "        output_delta = output_error  # Assuming derivative of linear output layer is 1\n",
        "\n",
        "        hidden_error = [sum([output_delta[j] * self.weights2[i][j] for j in range(len(output_delta))]) for i in range(len(self.hidden))]  # Error in hidden layer\n",
        "        hidden_delta = [hidden_error[i] * self.relu_derivative([self.hidden[i]])[0] for i in range(len(hidden_error))]\n",
        "\n",
        "        # Update weights and biases (Gradient Descent)\n",
        "        for i in range(len(self.weights1)):\n",
        "            for j in range(len(self.weights1[0])):\n",
        "                self.weights1[i][j] += learning_rate * hidden_delta[j] * self.input[i]\n",
        "        for i in range(len(self.weights2)):\n",
        "            for j in range(len(self.weights2[0])):\n",
        "                self.weights2[i][j] += learning_rate * output_delta[j] * self.hidden[i]\n",
        "\n",
        "        for j in range(len(self.bias1)):\n",
        "            self.bias1[j] += learning_rate * hidden_delta[j]\n",
        "        for j in range(len(self.bias2)):\n",
        "            self.bias2[j] += learning_rate * output_delta[j]\n",
        "\n",
        "\n",
        "# Initialize neural networks for prediction and target\n",
        "input_size = 5  # The size of the state space (one-hot encoded state)\n",
        "hidden_size = 10  # Number of neurons in the hidden layer\n",
        "output_size = 2  # Two possible actions (left or right)\n",
        "\n",
        "prediction_network = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "target_network = NeuralNetwork(input_size, hidden_size, output_size)  # Target network initialized the same way\n",
        "\n",
        "# Function to copy weights from prediction to target network\n",
        "def update_target_network():\n",
        "    target_network.weights1 = [row[:] for row in prediction_network.weights1]\n",
        "    target_network.weights2 = [row[:] for row in prediction_network.weights2]\n",
        "    target_network.bias1 = prediction_network.bias1[:]\n",
        "    target_network.bias2 = prediction_network.bias2[:]\n",
        "\n",
        "# One-hot encode the state\n",
        "def one_hot(state, size):\n",
        "    vec = [0] * size\n",
        "    vec[state] = 1\n",
        "    return vec\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.01  # Learning rate for neural network\n",
        "gamma = 0.9   # Discount factor\n",
        "epsilon = 1.0  # Exploration rate (starts high, will decay)\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.995\n",
        "num_episodes = 1000\n",
        "target_update_freq = 10  # Update target network every 10 episodes\n",
        "\n",
        "# Function to choose action based on epsilon-greedy policy\n",
        "def choose_action(state):\n",
        "    if random.random() < epsilon:  # Explore\n",
        "        return random.choice([0, 1])  # Random action\n",
        "    else:  # Exploit\n",
        "        q_values = prediction_network.forward(one_hot(state, input_size))\n",
        "        return q_values.index(max(q_values))  # Choose action with the highest Q-value\n",
        "\n",
        "# Function to calculate target Q-value\n",
        "def calculate_target(reward, next_state, done):\n",
        "    if done:\n",
        "        return reward  # No future rewards if done\n",
        "    else:\n",
        "        next_q_values = target_network.forward(one_hot(next_state, input_size))\n",
        "        return reward + gamma * max(next_q_values)  # Bellman equation\n",
        "\n",
        "# Training the agent with neural networks\n",
        "env = SimpleEnv()\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = choose_action(state)\n",
        "        next_state, reward, done = env.step(action)\n",
        "\n",
        "        # Calculate target Q-value\n",
        "        target_q = prediction_network.forward(one_hot(state, input_size))\n",
        "        target_q[action] = calculate_target(reward, next_state, done)  # Update the Q-value for the action taken\n",
        "\n",
        "        # Train the prediction network using backpropagation\n",
        "        prediction_network.backward(target_q, alpha)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    # Update target network every few episodes\n",
        "    if episode % target_update_freq == 0:\n",
        "        update_target_network()\n",
        "\n",
        "    # Decay epsilon to reduce exploration over time\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "# After training, print the learned Q-values for each state\n",
        "for state in range(5):\n",
        "    print(f\"State {state}: {prediction_network.forward(one_hot(state, input_size))}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UxXt_84RdbAv",
        "outputId": "7d5e2786-a8b3-4831-8553-9768bb442c0c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State 0: [4.105260622577344, 5.509370241111424]\n",
            "State 1: [4.178621597284838, 5.6777461169403]\n",
            "State 2: [3.5691402871591418, 4.5085546634293685]\n",
            "State 3: [3.8566815090198974, 4.986245968318332]\n",
            "State 4: [4.0941129130097345, 5.429921617365218]\n"
          ]
        }
      ]
    }
  ]
}